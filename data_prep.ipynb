{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adatok előkészítése - notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Feladat:\n",
    "Adatgyűjtés és előkészítés: Termékértékelési adatok letöltése egy nyílt adatforrásból (pl. Kaggle vagy Amazon API), és előfeldolgozás (szövegbeli zajok eltávolítása, tokenizálás stb.)\n",
    "\n",
    "##### Adathalmaz elérési link: https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Szükséges könyvtárak importálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\vani\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vani\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vani\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\vani\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\vani\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vani\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\Vani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adathalmazok betöltése, adatok megtekintése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'train.csv'\n",
    "test_file = 'test.csv'\n",
    "\n",
    "path = input('Path for the files: ')\n",
    "\n",
    "file_path = (os.path.join(path, file)).replace('\\\\', '/')\n",
    "test_file_path = (os.path.join(path, test_file)).replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path, names=['sentiment', 'title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer</td>\n",
       "      <td>This sound track was beautiful! It paints the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>I am quite sure any of you actually taking the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Buyer beware</td>\n",
       "      <td>This is a self-published book, and if you want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Glorious story</td>\n",
       "      <td>I loved Whisper of the wicked saints. The stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>A FIVE STAR BOOK</td>\n",
       "      <td>I just finished reading Whisper of the Wicked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Whispers of the Wicked Saints</td>\n",
       "      <td>This was a easy to read book that made me want...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              title  \\\n",
       "0          2                     Stuning even for the non-gamer   \n",
       "1          2              The best soundtrack ever to anything.   \n",
       "2          2                                           Amazing!   \n",
       "3          2                               Excellent Soundtrack   \n",
       "4          2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "5          2                            an absolute masterpiece   \n",
       "6          1                                       Buyer beware   \n",
       "7          2                                     Glorious story   \n",
       "8          2                                   A FIVE STAR BOOK   \n",
       "9          2                      Whispers of the Wicked Saints   \n",
       "\n",
       "                                                text  \n",
       "0  This sound track was beautiful! It paints the ...  \n",
       "1  I'm reading a lot of reviews saying that this ...  \n",
       "2  This soundtrack is my favorite music of all ti...  \n",
       "3  I truly like this soundtrack and I enjoy video...  \n",
       "4  If you've played the game, you know how divine...  \n",
       "5  I am quite sure any of you actually taking the...  \n",
       "6  This is a self-published book, and if you want...  \n",
       "7  I loved Whisper of the wicked saints. The stor...  \n",
       "8  I just finished reading Whisper of the Wicked ...  \n",
       "9  This was a easy to read book that made me want...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1800000\n",
       "1    1800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nan értékek detektálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment     0\n",
       "title        77\n",
       "text          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26554</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What separates this band from Evanescence (bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26827</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Falkenbach returns with more of the Viking/Fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36598</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I returned this because I received the same on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132358</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am a Shakespeare buff, so I didn't find this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134465</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Goes at quite a steady pace, however, this is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292195</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Last Van Gogh, is a compelling, visually p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293615</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can't write a review- it's been a month since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3403351</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It is not a game. It is only a memory cardIt w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493132</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Al Spath's diary is a must for all poker playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3575894</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I got on a short story kick after reading Jhum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment title                                               text\n",
       "26554            1   NaN  What separates this band from Evanescence (bes...\n",
       "26827            2   NaN  Falkenbach returns with more of the Viking/Fol...\n",
       "36598            2   NaN  I returned this because I received the same on...\n",
       "132358           2   NaN  I am a Shakespeare buff, so I didn't find this...\n",
       "134465           2   NaN  Goes at quite a steady pace, however, this is ...\n",
       "...            ...   ...                                                ...\n",
       "3292195          2   NaN  The Last Van Gogh, is a compelling, visually p...\n",
       "3293615          1   NaN  Can't write a review- it's been a month since ...\n",
       "3403351          1   NaN  It is not a game. It is only a memory cardIt w...\n",
       "3493132          2   NaN  Al Spath's diary is a must for all poker playe...\n",
       "3575894          1   NaN  I got on a short story kick after reading Jhum...\n",
       "\n",
       "[77 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Azon sorok kiíratása amik tartalamnak Nan értéket - mivel rengeteg adatunk van, ezt a 77 sort törölhetjük\n",
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adat előkészítés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nan értéket tartalmazó sorok eltávolítása\n",
    "* Szöveg kisbetűssé alakítása\n",
    "* Speciális karakterek (pl. írásjelek !,?) eltávolítása\n",
    "* Számok eltávolítása\n",
    "* Felesleges szóközök eltávolítása\n",
    "* Tokenizálás\n",
    "* Stop szavak eltávolítása\n",
    "* Lemmatizálás és szótőkeresés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan értéket tartalmazó sorok eltávolítása "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3599923, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szöbeg tisztítása regex-el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    text = text.lower()  # szöveg kisbetűssé alakítása\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # írásjelek eltávolítása\n",
    "    text = re.sub(r'\\d+', '', text)  # számok eltávolítása\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # extra szóközök eltávolítása\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data['title'].apply(prepare_text)\n",
    "data['text'] = data['text'].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>stuning even for the nongamer</td>\n",
       "      <td>this sound track was beautiful it paints the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>the best soundtrack ever to anything</td>\n",
       "      <td>im reading a lot of reviews saying that this i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>amazing</td>\n",
       "      <td>this soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>excellent soundtrack</td>\n",
       "      <td>i truly like this soundtrack and i enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>remember pull your jaw off the floor after hea...</td>\n",
       "      <td>if youve played the game you know how divine t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>i am quite sure any of you actually taking the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>this is a selfpublished book and if you want t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>glorious story</td>\n",
       "      <td>i loved whisper of the wicked saints the story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>a five star book</td>\n",
       "      <td>i just finished reading whisper of the wicked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>whispers of the wicked saints</td>\n",
       "      <td>this was a easy to read book that made me want...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              title  \\\n",
       "0          2                      stuning even for the nongamer   \n",
       "1          2               the best soundtrack ever to anything   \n",
       "2          2                                            amazing   \n",
       "3          2                               excellent soundtrack   \n",
       "4          2  remember pull your jaw off the floor after hea...   \n",
       "5          2                            an absolute masterpiece   \n",
       "6          1                                       buyer beware   \n",
       "7          2                                     glorious story   \n",
       "8          2                                   a five star book   \n",
       "9          2                      whispers of the wicked saints   \n",
       "\n",
       "                                                text  \n",
       "0  this sound track was beautiful it paints the s...  \n",
       "1  im reading a lot of reviews saying that this i...  \n",
       "2  this soundtrack is my favorite music of all ti...  \n",
       "3  i truly like this soundtrack and i enjoy video...  \n",
       "4  if youve played the game you know how divine t...  \n",
       "5  i am quite sure any of you actually taking the...  \n",
       "6  this is a selfpublished book and if you want t...  \n",
       "7  i loved whisper of the wicked saints the story...  \n",
       "8  i just finished reading whisper of the wicked ...  \n",
       "9  this was a easy to read book that made me want...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizálás Stop szavak eltávolítása és Lemmatizálás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global stop_words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "global lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_stop_words_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    final_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_tokens'] = data['title'].apply(tokenizer_stop_words_lemmatize)\n",
    "data['text_tokens'] = data['text'].apply(tokenizer_stop_words_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>stuning even for the nongamer</td>\n",
       "      <td>this sound track was beautiful it paints the s...</td>\n",
       "      <td>[stuning, even, nongamer]</td>\n",
       "      <td>[sound, track, beautiful, paint, senery, mind,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>the best soundtrack ever to anything</td>\n",
       "      <td>im reading a lot of reviews saying that this i...</td>\n",
       "      <td>[best, soundtrack, ever, anything]</td>\n",
       "      <td>[im, reading, lot, review, saying, best, game,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>amazing</td>\n",
       "      <td>this soundtrack is my favorite music of all ti...</td>\n",
       "      <td>[amazing]</td>\n",
       "      <td>[soundtrack, favorite, music, time, hand, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>excellent soundtrack</td>\n",
       "      <td>i truly like this soundtrack and i enjoy video...</td>\n",
       "      <td>[excellent, soundtrack]</td>\n",
       "      <td>[truly, like, soundtrack, enjoy, video, game, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>remember pull your jaw off the floor after hea...</td>\n",
       "      <td>if youve played the game you know how divine t...</td>\n",
       "      <td>[remember, pull, jaw, floor, hearing]</td>\n",
       "      <td>[youve, played, game, know, divine, music, eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>an absolute masterpiece</td>\n",
       "      <td>i am quite sure any of you actually taking the...</td>\n",
       "      <td>[absolute, masterpiece]</td>\n",
       "      <td>[quite, sure, actually, taking, time, read, pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>this is a selfpublished book and if you want t...</td>\n",
       "      <td>[buyer, beware]</td>\n",
       "      <td>[selfpublished, book, want, know, whyread, par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>glorious story</td>\n",
       "      <td>i loved whisper of the wicked saints the story...</td>\n",
       "      <td>[glorious, story]</td>\n",
       "      <td>[loved, whisper, wicked, saint, story, amazing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>a five star book</td>\n",
       "      <td>i just finished reading whisper of the wicked ...</td>\n",
       "      <td>[five, star, book]</td>\n",
       "      <td>[finished, reading, whisper, wicked, saint, fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>whispers of the wicked saints</td>\n",
       "      <td>this was a easy to read book that made me want...</td>\n",
       "      <td>[whisper, wicked, saint]</td>\n",
       "      <td>[easy, read, book, made, want, keep, reading, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              title  \\\n",
       "0          2                      stuning even for the nongamer   \n",
       "1          2               the best soundtrack ever to anything   \n",
       "2          2                                            amazing   \n",
       "3          2                               excellent soundtrack   \n",
       "4          2  remember pull your jaw off the floor after hea...   \n",
       "5          2                            an absolute masterpiece   \n",
       "6          1                                       buyer beware   \n",
       "7          2                                     glorious story   \n",
       "8          2                                   a five star book   \n",
       "9          2                      whispers of the wicked saints   \n",
       "\n",
       "                                                text  \\\n",
       "0  this sound track was beautiful it paints the s...   \n",
       "1  im reading a lot of reviews saying that this i...   \n",
       "2  this soundtrack is my favorite music of all ti...   \n",
       "3  i truly like this soundtrack and i enjoy video...   \n",
       "4  if youve played the game you know how divine t...   \n",
       "5  i am quite sure any of you actually taking the...   \n",
       "6  this is a selfpublished book and if you want t...   \n",
       "7  i loved whisper of the wicked saints the story...   \n",
       "8  i just finished reading whisper of the wicked ...   \n",
       "9  this was a easy to read book that made me want...   \n",
       "\n",
       "                            title_tokens  \\\n",
       "0              [stuning, even, nongamer]   \n",
       "1     [best, soundtrack, ever, anything]   \n",
       "2                              [amazing]   \n",
       "3                [excellent, soundtrack]   \n",
       "4  [remember, pull, jaw, floor, hearing]   \n",
       "5                [absolute, masterpiece]   \n",
       "6                        [buyer, beware]   \n",
       "7                      [glorious, story]   \n",
       "8                     [five, star, book]   \n",
       "9               [whisper, wicked, saint]   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [sound, track, beautiful, paint, senery, mind,...  \n",
       "1  [im, reading, lot, review, saying, best, game,...  \n",
       "2  [soundtrack, favorite, music, time, hand, inte...  \n",
       "3  [truly, like, soundtrack, enjoy, video, game, ...  \n",
       "4  [youve, played, game, know, divine, music, eve...  \n",
       "5  [quite, sure, actually, taking, time, read, pl...  \n",
       "6  [selfpublished, book, want, know, whyread, par...  \n",
       "7  [loved, whisper, wicked, saint, story, amazing...  \n",
       "8  [finished, reading, whisper, wicked, saint, fe...  \n",
       "9  [easy, read, book, made, want, keep, reading, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3599990</th>\n",
       "      <td>2</td>\n",
       "      <td>buy this cd and youll thank yourself</td>\n",
       "      <td>tyler hiltona name you might not know now but ...</td>\n",
       "      <td>[buy, cd, youll, thank]</td>\n",
       "      <td>[tyler, hiltona, name, might, know, sure, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599991</th>\n",
       "      <td>2</td>\n",
       "      <td>tyler rocks</td>\n",
       "      <td>there is only one word to describe tyler hilto...</td>\n",
       "      <td>[tyler, rock]</td>\n",
       "      <td>[one, word, describe, tyler, hiltontalent, lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599992</th>\n",
       "      <td>2</td>\n",
       "      <td>awesome</td>\n",
       "      <td>absolutely amazing so relieving of my neck pai...</td>\n",
       "      <td>[awesome]</td>\n",
       "      <td>[absolutely, amazing, relieving, neck, pain, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599993</th>\n",
       "      <td>1</td>\n",
       "      <td>what a slap in the face to masami ueda</td>\n",
       "      <td>do not buy this cd ever this was probably just...</td>\n",
       "      <td>[slap, face, masami, ueda]</td>\n",
       "      <td>[buy, cd, ever, probably, released, test, see,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599994</th>\n",
       "      <td>1</td>\n",
       "      <td>too simplistic</td>\n",
       "      <td>while mr harrison makes some extremely valid a...</td>\n",
       "      <td>[simplistic]</td>\n",
       "      <td>[mr, harrison, make, extremely, valid, argumen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599995</th>\n",
       "      <td>1</td>\n",
       "      <td>dont do it</td>\n",
       "      <td>the high chair looks great when it first comes...</td>\n",
       "      <td>[dont]</td>\n",
       "      <td>[high, chair, look, great, first, come, box, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599996</th>\n",
       "      <td>1</td>\n",
       "      <td>looks nice low functionality</td>\n",
       "      <td>i have used this highchair for kids now and fi...</td>\n",
       "      <td>[look, nice, low, functionality]</td>\n",
       "      <td>[used, highchair, kid, finally, decided, sell,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599997</th>\n",
       "      <td>1</td>\n",
       "      <td>compact but hard to clean</td>\n",
       "      <td>we have a small house and really wanted two of...</td>\n",
       "      <td>[compact, hard, clean]</td>\n",
       "      <td>[small, house, really, wanted, two, high, chai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599998</th>\n",
       "      <td>1</td>\n",
       "      <td>what is it saying</td>\n",
       "      <td>not sure what this book is supposed to be it i...</td>\n",
       "      <td>[saying]</td>\n",
       "      <td>[sure, book, supposed, really, rehash, old, id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599999</th>\n",
       "      <td>2</td>\n",
       "      <td>makes my blood run redwhiteandblue</td>\n",
       "      <td>i agree that every american should read this b...</td>\n",
       "      <td>[make, blood, run, redwhiteandblue]</td>\n",
       "      <td>[agree, every, american, read, book, everybody...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                   title  \\\n",
       "3599990          2    buy this cd and youll thank yourself   \n",
       "3599991          2                             tyler rocks   \n",
       "3599992          2                                 awesome   \n",
       "3599993          1  what a slap in the face to masami ueda   \n",
       "3599994          1                          too simplistic   \n",
       "3599995          1                              dont do it   \n",
       "3599996          1            looks nice low functionality   \n",
       "3599997          1               compact but hard to clean   \n",
       "3599998          1                       what is it saying   \n",
       "3599999          2      makes my blood run redwhiteandblue   \n",
       "\n",
       "                                                      text  \\\n",
       "3599990  tyler hiltona name you might not know now but ...   \n",
       "3599991  there is only one word to describe tyler hilto...   \n",
       "3599992  absolutely amazing so relieving of my neck pai...   \n",
       "3599993  do not buy this cd ever this was probably just...   \n",
       "3599994  while mr harrison makes some extremely valid a...   \n",
       "3599995  the high chair looks great when it first comes...   \n",
       "3599996  i have used this highchair for kids now and fi...   \n",
       "3599997  we have a small house and really wanted two of...   \n",
       "3599998  not sure what this book is supposed to be it i...   \n",
       "3599999  i agree that every american should read this b...   \n",
       "\n",
       "                                title_tokens  \\\n",
       "3599990              [buy, cd, youll, thank]   \n",
       "3599991                        [tyler, rock]   \n",
       "3599992                            [awesome]   \n",
       "3599993           [slap, face, masami, ueda]   \n",
       "3599994                         [simplistic]   \n",
       "3599995                               [dont]   \n",
       "3599996     [look, nice, low, functionality]   \n",
       "3599997               [compact, hard, clean]   \n",
       "3599998                             [saying]   \n",
       "3599999  [make, blood, run, redwhiteandblue]   \n",
       "\n",
       "                                               text_tokens  \n",
       "3599990  [tyler, hiltona, name, might, know, sure, with...  \n",
       "3599991  [one, word, describe, tyler, hiltontalent, lov...  \n",
       "3599992  [absolutely, amazing, relieving, neck, pain, i...  \n",
       "3599993  [buy, cd, ever, probably, released, test, see,...  \n",
       "3599994  [mr, harrison, make, extremely, valid, argumen...  \n",
       "3599995  [high, chair, look, great, first, come, box, h...  \n",
       "3599996  [used, highchair, kid, finally, decided, sell,...  \n",
       "3599997  [small, house, really, wanted, two, high, chai...  \n",
       "3599998  [sure, book, supposed, really, rehash, old, id...  \n",
       "3599999  [agree, every, american, read, book, everybody...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adatok elmentése csv fájlba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_file = 'cleaned_train.csv'\n",
    "dest_test_file = 'cleaned_test.csv'\n",
    "\n",
    "dest_file_path = os.path.join(path, dest_file)\n",
    "data.to_csv(dest_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
